{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"D:\\Karan\\ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"bank_xgb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>age</th>\n",
       "      <th>balance</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>previous</th>\n",
       "      <th>recent_pdays</th>\n",
       "      <th>deposit_cat</th>\n",
       "      <th>job_Pink-collar</th>\n",
       "      <th>job_White-collar</th>\n",
       "      <th>...</th>\n",
       "      <th>marital_married</th>\n",
       "      <th>marital_single</th>\n",
       "      <th>education_secondary</th>\n",
       "      <th>education_tertiary</th>\n",
       "      <th>education_unknown</th>\n",
       "      <th>default_yes</th>\n",
       "      <th>housing_yes</th>\n",
       "      <th>loan_yes</th>\n",
       "      <th>poutcome_others</th>\n",
       "      <th>poutcome_success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>2343</td>\n",
       "      <td>1042</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>45</td>\n",
       "      <td>1467</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>1270</td>\n",
       "      <td>1389</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "      <td>2476</td>\n",
       "      <td>579</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>184</td>\n",
       "      <td>673</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  age  balance  duration  campaign  previous  recent_pdays  \\\n",
       "0           0   59     2343      1042         1         0        0.0001   \n",
       "1           1   56       45      1467         1         0        0.0001   \n",
       "2           2   41     1270      1389         1         0        0.0001   \n",
       "3           3   55     2476       579         1         0        0.0001   \n",
       "4           4   54      184       673         2         0        0.0001   \n",
       "\n",
       "   deposit_cat  job_Pink-collar  job_White-collar  ...  marital_married  \\\n",
       "0            1                0                 1  ...                1   \n",
       "1            1                0                 1  ...                1   \n",
       "2            1                0                 0  ...                1   \n",
       "3            1                1                 0  ...                1   \n",
       "4            1                0                 1  ...                1   \n",
       "\n",
       "   marital_single  education_secondary  education_tertiary  education_unknown  \\\n",
       "0               0                    1                   0                  0   \n",
       "1               0                    1                   0                  0   \n",
       "2               0                    1                   0                  0   \n",
       "3               0                    1                   0                  0   \n",
       "4               0                    0                   1                  0   \n",
       "\n",
       "   default_yes  housing_yes  loan_yes  poutcome_others  poutcome_success  \n",
       "0            0            1         0                1                 0  \n",
       "1            0            0         0                1                 0  \n",
       "2            0            1         0                1                 0  \n",
       "3            0            1         0                1                 0  \n",
       "4            0            0         0                1                 0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'age', 'balance', 'duration', 'campaign', 'previous',\n",
       "       'recent_pdays', 'deposit_cat', 'job_Pink-collar', 'job_White-collar',\n",
       "       'job_blue-collar', 'job_self-depend', 'job_technician',\n",
       "       'marital_married', 'marital_single', 'education_secondary',\n",
       "       'education_tertiary', 'education_unknown', 'default_yes', 'housing_yes',\n",
       "       'loan_yes', 'poutcome_others', 'poutcome_success'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"Unnamed: 0\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>balance</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>previous</th>\n",
       "      <th>recent_pdays</th>\n",
       "      <th>deposit_cat</th>\n",
       "      <th>job_Pink-collar</th>\n",
       "      <th>job_White-collar</th>\n",
       "      <th>job_blue-collar</th>\n",
       "      <th>...</th>\n",
       "      <th>marital_married</th>\n",
       "      <th>marital_single</th>\n",
       "      <th>education_secondary</th>\n",
       "      <th>education_tertiary</th>\n",
       "      <th>education_unknown</th>\n",
       "      <th>default_yes</th>\n",
       "      <th>housing_yes</th>\n",
       "      <th>loan_yes</th>\n",
       "      <th>poutcome_others</th>\n",
       "      <th>poutcome_success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59</td>\n",
       "      <td>2343</td>\n",
       "      <td>1042</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56</td>\n",
       "      <td>45</td>\n",
       "      <td>1467</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>1270</td>\n",
       "      <td>1389</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>2476</td>\n",
       "      <td>579</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>184</td>\n",
       "      <td>673</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  balance  duration  campaign  previous  recent_pdays  deposit_cat  \\\n",
       "0   59     2343      1042         1         0        0.0001            1   \n",
       "1   56       45      1467         1         0        0.0001            1   \n",
       "2   41     1270      1389         1         0        0.0001            1   \n",
       "3   55     2476       579         1         0        0.0001            1   \n",
       "4   54      184       673         2         0        0.0001            1   \n",
       "\n",
       "   job_Pink-collar  job_White-collar  job_blue-collar  ...  marital_married  \\\n",
       "0                0                 1                0  ...                1   \n",
       "1                0                 1                0  ...                1   \n",
       "2                0                 0                0  ...                1   \n",
       "3                1                 0                0  ...                1   \n",
       "4                0                 1                0  ...                1   \n",
       "\n",
       "   marital_single  education_secondary  education_tertiary  education_unknown  \\\n",
       "0               0                    1                   0                  0   \n",
       "1               0                    1                   0                  0   \n",
       "2               0                    1                   0                  0   \n",
       "3               0                    1                   0                  0   \n",
       "4               0                    0                   1                  0   \n",
       "\n",
       "   default_yes  housing_yes  loan_yes  poutcome_others  poutcome_success  \n",
       "0            0            1         0                1                 0  \n",
       "1            0            0         0                1                 0  \n",
       "2            0            1         0                1                 0  \n",
       "3            0            1         0                1                 0  \n",
       "4            0            0         0                1                 0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'balance', 'duration', 'campaign', 'previous', 'recent_pdays',\n",
       "       'deposit_cat', 'job_Pink-collar', 'job_White-collar', 'job_blue-collar',\n",
       "       'job_self-depend', 'job_technician', 'marital_married',\n",
       "       'marital_single', 'education_secondary', 'education_tertiary',\n",
       "       'education_unknown', 'default_yes', 'housing_yes', 'loan_yes',\n",
       "       'poutcome_others', 'poutcome_success'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data[\"deposit_cat\"]\n",
    "X=data.drop([\"deposit_cat\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.479247\n",
      "         Iterations 7\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "gh=sm.logit(\"deposit_cat ~ age+balance+duration+balance+recent_pdays+housing_yes+poutcome_success\",data).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:            deposit_cat   No. Observations:                11162\n",
      "Model:                          Logit   Df Residuals:                    11155\n",
      "Method:                           MLE   Df Model:                            6\n",
      "Date:                Sat, 26 Feb 2022   Pseudo R-squ.:                  0.3072\n",
      "Time:                        15:26:30   Log-Likelihood:                -5349.3\n",
      "converged:                       True   LL-Null:                       -7721.6\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "====================================================================================\n",
      "                       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------\n",
      "Intercept           -1.3974      0.097    -14.457      0.000      -1.587      -1.208\n",
      "age                 -0.0037      0.002     -1.818      0.069      -0.008       0.000\n",
      "balance           4.777e-05    8.3e-06      5.754      0.000    3.15e-05     6.4e-05\n",
      "duration             0.0049      0.000     43.398      0.000       0.005       0.005\n",
      "recent_pdays         0.5159      0.969      0.532      0.595      -1.384       2.416\n",
      "housing_yes         -1.1121      0.050    -22.153      0.000      -1.210      -1.014\n",
      "poutcome_success     2.8666      0.116     24.713      0.000       2.639       3.094\n",
      "====================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(gh.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install abcXYZ\n",
    "import abcXYZ\n",
    "abcXYZ.ab(4,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn import svm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(X,y, test_size=.2, random_state=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=StandardScaler()\n",
    "sc_fit=sc.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc=sc_fit.transform(X_train)\n",
    "X_test_sc=sc_fit.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(X_train_sc, columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc_df=pd.DataFrame(X_train_sc, columns=X_train.columns)\n",
    "X_test_sc_df=pd.DataFrame(X_test_sc, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_sc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.svm in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.svm - The :mod:`sklearn.svm` module includes Support Vector Machine algorithms.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _base\n",
      "    _bounds\n",
      "    _classes\n",
      "    _liblinear\n",
      "    _libsvm\n",
      "    _libsvm_sparse\n",
      "    _newrand\n",
      "    setup\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        sklearn.svm._classes.LinearSVC(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.OutlierMixin(builtins.object)\n",
      "        sklearn.svm._classes.OneClassSVM(sklearn.base.OutlierMixin, sklearn.svm._base.BaseLibSVM)\n",
      "    sklearn.base.RegressorMixin(builtins.object)\n",
      "        sklearn.svm._classes.LinearSVR(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.svm._classes.NuSVR(sklearn.base.RegressorMixin, sklearn.svm._base.BaseLibSVM)\n",
      "        sklearn.svm._classes.SVR(sklearn.base.RegressorMixin, sklearn.svm._base.BaseLibSVM)\n",
      "    sklearn.linear_model._base.LinearClassifierMixin(sklearn.base.ClassifierMixin)\n",
      "        sklearn.svm._classes.LinearSVC(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.linear_model._base.LinearModel(sklearn.base.BaseEstimator)\n",
      "        sklearn.svm._classes.LinearSVR(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "    sklearn.linear_model._base.SparseCoefMixin(builtins.object)\n",
      "        sklearn.svm._classes.LinearSVC(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.svm._base.BaseLibSVM(sklearn.base.BaseEstimator)\n",
      "        sklearn.svm._classes.NuSVR(sklearn.base.RegressorMixin, sklearn.svm._base.BaseLibSVM)\n",
      "        sklearn.svm._classes.OneClassSVM(sklearn.base.OutlierMixin, sklearn.svm._base.BaseLibSVM)\n",
      "        sklearn.svm._classes.SVR(sklearn.base.RegressorMixin, sklearn.svm._base.BaseLibSVM)\n",
      "    sklearn.svm._base.BaseSVC(sklearn.base.ClassifierMixin, sklearn.svm._base.BaseLibSVM)\n",
      "        sklearn.svm._classes.NuSVC\n",
      "        sklearn.svm._classes.SVC\n",
      "    \n",
      "    class LinearSVC(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      "     |  Linear Support Vector Classification.\n",
      "     |  \n",
      "     |  Similar to SVC with parameter kernel='linear', but implemented in terms of\n",
      "     |  liblinear rather than libsvm, so it has more flexibility in the choice of\n",
      "     |  penalties and loss functions and should scale better to large numbers of\n",
      "     |  samples.\n",
      "     |  \n",
      "     |  This class supports both dense and sparse input and the multiclass support\n",
      "     |  is handled according to a one-vs-the-rest scheme.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <svm_classification>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  penalty : {'l1', 'l2'}, default='l2'\n",
      "     |      Specifies the norm used in the penalization. The 'l2'\n",
      "     |      penalty is the standard used in SVC. The 'l1' leads to ``coef_``\n",
      "     |      vectors that are sparse.\n",
      "     |  \n",
      "     |  loss : {'hinge', 'squared_hinge'}, default='squared_hinge'\n",
      "     |      Specifies the loss function. 'hinge' is the standard SVM loss\n",
      "     |      (used e.g. by the SVC class) while 'squared_hinge' is the\n",
      "     |      square of the hinge loss. The combination of ``penalty='l1'``\n",
      "     |      and ``loss='hinge'`` is not supported.\n",
      "     |  \n",
      "     |  dual : bool, default=True\n",
      "     |      Select the algorithm to either solve the dual or primal\n",
      "     |      optimization problem. Prefer dual=False when n_samples > n_features.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Tolerance for stopping criteria.\n",
      "     |  \n",
      "     |  C : float, default=1.0\n",
      "     |      Regularization parameter. The strength of the regularization is\n",
      "     |      inversely proportional to C. Must be strictly positive.\n",
      "     |  \n",
      "     |  multi_class : {'ovr', 'crammer_singer'}, default='ovr'\n",
      "     |      Determines the multi-class strategy if `y` contains more than\n",
      "     |      two classes.\n",
      "     |      ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while\n",
      "     |      ``\"crammer_singer\"`` optimizes a joint objective over all classes.\n",
      "     |      While `crammer_singer` is interesting from a theoretical perspective\n",
      "     |      as it is consistent, it is seldom used in practice as it rarely leads\n",
      "     |      to better accuracy and is more expensive to compute.\n",
      "     |      If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual\n",
      "     |      will be ignored.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be already centered).\n",
      "     |  \n",
      "     |  intercept_scaling : float, default=1\n",
      "     |      When self.fit_intercept is True, instance vector x becomes\n",
      "     |      ``[x, self.intercept_scaling]``,\n",
      "     |      i.e. a \"synthetic\" feature with constant value equals to\n",
      "     |      intercept_scaling is appended to the instance vector.\n",
      "     |      The intercept becomes intercept_scaling * synthetic feature weight\n",
      "     |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      "     |      as all other features.\n",
      "     |      To lessen the effect of regularization on synthetic feature weight\n",
      "     |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', default=None\n",
      "     |      Set the parameter C of class i to ``class_weight[i]*C`` for\n",
      "     |      SVC. If not given, all classes are supposed to have\n",
      "     |      weight one.\n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Enable verbose output. Note that this setting takes advantage of a\n",
      "     |      per-process runtime setting in liblinear that, if enabled, may not work\n",
      "     |      properly in a multithreaded context.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the pseudo random number generation for shuffling the data for\n",
      "     |      the dual coordinate descent (if ``dual=True``). When ``dual=False`` the\n",
      "     |      underlying implementation of :class:`LinearSVC` is not random and\n",
      "     |      ``random_state`` has no effect on the results.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations to be run.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (1, n_features) if n_classes == 2             else (n_classes, n_features)\n",
      "     |      Weights assigned to the features (coefficients in the primal\n",
      "     |      problem). This is only available in the case of a linear kernel.\n",
      "     |  \n",
      "     |      ``coef_`` is a readonly property derived from ``raw_coef_`` that\n",
      "     |      follows the internal memory layout of liblinear.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The unique classes labels.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Maximum number of iterations run across all classes.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  SVC : Implementation of Support Vector Machine classifier using libsvm:\n",
      "     |      the kernel can be non-linear but its SMO algorithm does not\n",
      "     |      scale to large number of samples as LinearSVC does.\n",
      "     |  \n",
      "     |      Furthermore SVC multi-class mode is implemented using one\n",
      "     |      vs one scheme while LinearSVC uses one vs the rest. It is\n",
      "     |      possible to implement one vs the rest with SVC by using the\n",
      "     |      :class:`~sklearn.multiclass.OneVsRestClassifier` wrapper.\n",
      "     |  \n",
      "     |      Finally SVC can fit dense data without memory copy if the input\n",
      "     |      is C-contiguous. Sparse data will still incur memory copy though.\n",
      "     |  \n",
      "     |  sklearn.linear_model.SGDClassifier : SGDClassifier can optimize the same\n",
      "     |      cost function as LinearSVC\n",
      "     |      by adjusting the penalty and loss parameters. In addition it requires\n",
      "     |      less memory, allows incremental (online) learning, and implements\n",
      "     |      various loss functions and regularization regimes.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The underlying C implementation uses a random number generator to\n",
      "     |  select features when fitting the model. It is thus not uncommon\n",
      "     |  to have slightly different results for the same input data. If\n",
      "     |  that happens, try with a smaller ``tol`` parameter.\n",
      "     |  \n",
      "     |  The underlying implementation, liblinear, uses a sparse internal\n",
      "     |  representation for the data that will incur a memory copy.\n",
      "     |  \n",
      "     |  Predict output may not match that of standalone liblinear in certain\n",
      "     |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      "     |  in the narrative documentation.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  `LIBLINEAR: A Library for Large Linear Classification\n",
      "     |  <https://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.svm import LinearSVC\n",
      "     |  >>> from sklearn.pipeline import make_pipeline\n",
      "     |  >>> from sklearn.preprocessing import StandardScaler\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> X, y = make_classification(n_features=4, random_state=0)\n",
      "     |  >>> clf = make_pipeline(StandardScaler(),\n",
      "     |  ...                     LinearSVC(random_state=0, tol=1e-5))\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "     |                  ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])\n",
      "     |  \n",
      "     |  >>> print(clf.named_steps['linearsvc'].coef_)\n",
      "     |  [[0.141...   0.526... 0.679... 0.493...]]\n",
      "     |  \n",
      "     |  >>> print(clf.named_steps['linearsvc'].intercept_)\n",
      "     |  [0.1693...]\n",
      "     |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LinearSVC\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, penalty='l2', loss='squared_hinge', *, dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vector, where n_samples in the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target vector relative to X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Array of weights that are assigned to individual\n",
      "     |          samples. If not provided,\n",
      "     |          then each sample is given unit weight.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          An instance of the estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is proportional to the signed\n",
      "     |      distance of that sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class LinearSVR(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  Linear Support Vector Regression.\n",
      "     |  \n",
      "     |  Similar to SVR with parameter kernel='linear', but implemented in terms of\n",
      "     |  liblinear rather than libsvm, so it has more flexibility in the choice of\n",
      "     |  penalties and loss functions and should scale better to large numbers of\n",
      "     |  samples.\n",
      "     |  \n",
      "     |  This class supports both dense and sparse input.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <svm_regression>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.16\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  epsilon : float, default=0.0\n",
      "     |      Epsilon parameter in the epsilon-insensitive loss function. Note\n",
      "     |      that the value of this parameter depends on the scale of the target\n",
      "     |      variable y. If unsure, set ``epsilon=0``.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Tolerance for stopping criteria.\n",
      "     |  \n",
      "     |  C : float, default=1.0\n",
      "     |      Regularization parameter. The strength of the regularization is\n",
      "     |      inversely proportional to C. Must be strictly positive.\n",
      "     |  \n",
      "     |  loss : {'epsilon_insensitive', 'squared_epsilon_insensitive'},             default='epsilon_insensitive'\n",
      "     |      Specifies the loss function. The epsilon-insensitive loss\n",
      "     |      (standard SVR) is the L1 loss, while the squared epsilon-insensitive\n",
      "     |      loss ('squared_epsilon_insensitive') is the L2 loss.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be already centered).\n",
      "     |  \n",
      "     |  intercept_scaling : float, default=1.\n",
      "     |      When self.fit_intercept is True, instance vector x becomes\n",
      "     |      [x, self.intercept_scaling],\n",
      "     |      i.e. a \"synthetic\" feature with constant value equals to\n",
      "     |      intercept_scaling is appended to the instance vector.\n",
      "     |      The intercept becomes intercept_scaling * synthetic feature weight\n",
      "     |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      "     |      as all other features.\n",
      "     |      To lessen the effect of regularization on synthetic feature weight\n",
      "     |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      "     |  \n",
      "     |  dual : bool, default=True\n",
      "     |      Select the algorithm to either solve the dual or primal\n",
      "     |      optimization problem. Prefer dual=False when n_samples > n_features.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Enable verbose output. Note that this setting takes advantage of a\n",
      "     |      per-process runtime setting in liblinear that, if enabled, may not work\n",
      "     |      properly in a multithreaded context.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the pseudo random number generation for shuffling the data.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations to be run.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_features) if n_classes == 2             else (n_classes, n_features)\n",
      "     |      Weights assigned to the features (coefficients in the primal\n",
      "     |      problem). This is only available in the case of a linear kernel.\n",
      "     |  \n",
      "     |      `coef_` is a readonly property derived from `raw_coef_` that\n",
      "     |      follows the internal memory layout of liblinear.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1) if n_classes == 2 else (n_classes)\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Maximum number of iterations run across all classes.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.svm import LinearSVR\n",
      "     |  >>> from sklearn.pipeline import make_pipeline\n",
      "     |  >>> from sklearn.preprocessing import StandardScaler\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(n_features=4, random_state=0)\n",
      "     |  >>> regr = make_pipeline(StandardScaler(),\n",
      "     |  ...                      LinearSVR(random_state=0, tol=1e-5))\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "     |                  ('linearsvr', LinearSVR(random_state=0, tol=1e-05))])\n",
      "     |  \n",
      "     |  >>> print(regr.named_steps['linearsvr'].coef_)\n",
      "     |  [18.582... 27.023... 44.357... 64.522...]\n",
      "     |  >>> print(regr.named_steps['linearsvr'].intercept_)\n",
      "     |  [-4...]\n",
      "     |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      "     |  [-2.384...]\n",
      "     |  \n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  LinearSVC : Implementation of Support Vector Machine classifier using the\n",
      "     |      same library as this class (liblinear).\n",
      "     |  \n",
      "     |  SVR : Implementation of Support Vector Machine regression using libsvm:\n",
      "     |      the kernel can be non-linear but its SMO algorithm does not\n",
      "     |      scale to large number of samples as LinearSVC does.\n",
      "     |  \n",
      "     |  sklearn.linear_model.SGDRegressor : SGDRegressor can optimize the same cost\n",
      "     |      function as LinearSVR\n",
      "     |      by adjusting the penalty and loss parameters. In addition it requires\n",
      "     |      less memory, allows incremental (online) learning, and implements\n",
      "     |      various loss functions and regularization regimes.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LinearSVR\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, epsilon=0.0, tol=0.0001, C=1.0, loss='epsilon_insensitive', fit_intercept=True, intercept_scaling=1.0, dual=True, verbose=0, random_state=None, max_iter=1000)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vector, where n_samples in the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target vector relative to X\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Array of weights that are assigned to individual\n",
      "     |          samples. If not provided,\n",
      "     |          then each sample is given unit weight.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          An instance of the estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class NuSVC(sklearn.svm._base.BaseSVC)\n",
      "     |  Nu-Support Vector Classification.\n",
      "     |  \n",
      "     |  Similar to SVC but uses a parameter to control the number of support\n",
      "     |  vectors.\n",
      "     |  \n",
      "     |  The implementation is based on libsvm.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <svm_classification>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  nu : float, default=0.5\n",
      "     |      An upper bound on the fraction of margin errors (see :ref:`User Guide\n",
      "     |      <nu_svc>`) and a lower bound of the fraction of support vectors.\n",
      "     |      Should be in the interval (0, 1].\n",
      "     |  \n",
      "     |  kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\n",
      "     |       Specifies the kernel type to be used in the algorithm.\n",
      "     |       It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
      "     |       a callable.\n",
      "     |       If none is given, 'rbf' will be used. If a callable is given it is\n",
      "     |       used to precompute the kernel matrix.\n",
      "     |  \n",
      "     |  degree : int, default=3\n",
      "     |      Degree of the polynomial kernel function ('poly').\n",
      "     |      Ignored by all other kernels.\n",
      "     |  \n",
      "     |  gamma : {'scale', 'auto'} or float, default='scale'\n",
      "     |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
      "     |  \n",
      "     |      - if ``gamma='scale'`` (default) is passed then it uses\n",
      "     |        1 / (n_features * X.var()) as value of gamma,\n",
      "     |      - if 'auto', uses 1 / n_features.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |         The default value of ``gamma`` changed from 'auto' to 'scale'.\n",
      "     |  \n",
      "     |  coef0 : float, default=0.0\n",
      "     |      Independent term in kernel function.\n",
      "     |      It is only significant in 'poly' and 'sigmoid'.\n",
      "     |  \n",
      "     |  shrinking : bool, default=True\n",
      "     |      Whether to use the shrinking heuristic.\n",
      "     |      See the :ref:`User Guide <shrinking_svm>`.\n",
      "     |  \n",
      "     |  probability : bool, default=False\n",
      "     |      Whether to enable probability estimates. This must be enabled prior\n",
      "     |      to calling `fit`, will slow down that method as it internally uses\n",
      "     |      5-fold cross-validation, and `predict_proba` may be inconsistent with\n",
      "     |      `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      Tolerance for stopping criterion.\n",
      "     |  \n",
      "     |  cache_size : float, default=200\n",
      "     |      Specify the size of the kernel cache (in MB).\n",
      "     |  \n",
      "     |  class_weight : {dict, 'balanced'}, default=None\n",
      "     |      Set the parameter C of class i to class_weight[i]*C for\n",
      "     |      SVC. If not given, all classes are supposed to have\n",
      "     |      weight one. The \"balanced\" mode uses the values of y to automatically\n",
      "     |      adjust weights inversely proportional to class frequencies as\n",
      "     |      ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |  verbose : bool, default=False\n",
      "     |      Enable verbose output. Note that this setting takes advantage of a\n",
      "     |      per-process runtime setting in libsvm that, if enabled, may not work\n",
      "     |      properly in a multithreaded context.\n",
      "     |  \n",
      "     |  max_iter : int, default=-1\n",
      "     |      Hard limit on iterations within solver, or -1 for no limit.\n",
      "     |  \n",
      "     |  decision_function_shape : {'ovo', 'ovr'}, default='ovr'\n",
      "     |      Whether to return a one-vs-rest ('ovr') decision function of shape\n",
      "     |      (n_samples, n_classes) as all other classifiers, or the original\n",
      "     |      one-vs-one ('ovo') decision function of libsvm which has shape\n",
      "     |      (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one\n",
      "     |      ('ovo') is always used as multi-class strategy. The parameter is\n",
      "     |      ignored for binary classification.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.19\n",
      "     |          decision_function_shape is 'ovr' by default.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *decision_function_shape='ovr'* is recommended.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.17\n",
      "     |         Deprecated *decision_function_shape='ovo' and None*.\n",
      "     |  \n",
      "     |  break_ties : bool, default=False\n",
      "     |      If true, ``decision_function_shape='ovr'``, and number of classes > 2,\n",
      "     |      :term:`predict` will break ties according to the confidence values of\n",
      "     |      :term:`decision_function`; otherwise the first class among the tied\n",
      "     |      classes is returned. Please note that breaking ties comes at a\n",
      "     |      relatively high computational cost compared to a simple predict.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the pseudo random number generation for shuffling the data for\n",
      "     |      probability estimates. Ignored when `probability` is False.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  class_weight_ : ndarray of shape (n_classes,)\n",
      "     |      Multipliers of parameter C of each class.\n",
      "     |      Computed based on the ``class_weight`` parameter.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The unique classes labels.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_classes * (n_classes -1) / 2, n_features)\n",
      "     |      Weights assigned to the features (coefficients in the primal\n",
      "     |      problem). This is only available in the case of a linear kernel.\n",
      "     |  \n",
      "     |      `coef_` is readonly property derived from `dual_coef_` and\n",
      "     |      `support_vectors_`.\n",
      "     |  \n",
      "     |  dual_coef_ : ndarray of shape (n_classes - 1, n_SV)\n",
      "     |      Dual coefficients of the support vector in the decision\n",
      "     |      function (see :ref:`sgd_mathematical_formulation`), multiplied by\n",
      "     |      their targets.\n",
      "     |      For multiclass, coefficient for all 1-vs-1 classifiers.\n",
      "     |      The layout of the coefficients in the multiclass case is somewhat\n",
      "     |      non-trivial. See the :ref:`multi-class section of the User Guide\n",
      "     |      <svm_multi_class>` for details.\n",
      "     |  \n",
      "     |  fit_status_ : int\n",
      "     |      0 if correctly fitted, 1 if the algorithm did not converge.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  support_ : ndarray of shape (n_SV,)\n",
      "     |      Indices of support vectors.\n",
      "     |  \n",
      "     |  support_vectors_ : ndarray of shape (n_SV, n_features)\n",
      "     |      Support vectors.\n",
      "     |  \n",
      "     |  n_support_ : ndarray of shape (n_classes,), dtype=int32\n",
      "     |      Number of support vectors for each class.\n",
      "     |  \n",
      "     |  fit_status_ : int\n",
      "     |      0 if correctly fitted, 1 if the algorithm did not converge.\n",
      "     |  \n",
      "     |  probA_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n",
      "     |  probB_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n",
      "     |      If `probability=True`, it corresponds to the parameters learned in\n",
      "     |      Platt scaling to produce probability estimates from decision values.\n",
      "     |      If `probability=False`, it's an empty array. Platt scaling uses the\n",
      "     |      logistic function\n",
      "     |      ``1 / (1 + exp(decision_value * probA_ + probB_))``\n",
      "     |      where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\n",
      "     |      more information on the multiclass case and training procedure see\n",
      "     |      section 8 of [1]_.\n",
      "     |  \n",
      "     |  shape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n",
      "     |      Array dimensions of training vector ``X``.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      "     |  >>> y = np.array([1, 1, 2, 2])\n",
      "     |  >>> from sklearn.pipeline import make_pipeline\n",
      "     |  >>> from sklearn.preprocessing import StandardScaler\n",
      "     |  >>> from sklearn.svm import NuSVC\n",
      "     |  >>> clf = make_pipeline(StandardScaler(), NuSVC())\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  Pipeline(steps=[('standardscaler', StandardScaler()), ('nusvc', NuSVC())])\n",
      "     |  >>> print(clf.predict([[-0.8, -1]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  SVC : Support Vector Machine for classification using libsvm.\n",
      "     |  \n",
      "     |  LinearSVC : Scalable linear Support Vector Machine for classification using\n",
      "     |      liblinear.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] `LIBSVM: A Library for Support Vector Machines\n",
      "     |      <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n",
      "     |  \n",
      "     |  .. [2] `Platt, John (1999). \"Probabilistic outputs for support vector\n",
      "     |      machines and comparison to regularizedlikelihood methods.\"\n",
      "     |      <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639>`_\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NuSVC\n",
      "     |      sklearn.svm._base.BaseSVC\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.svm._base.BaseLibSVM\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, nu=0.5, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.svm._base.BaseSVC:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Evaluates the decision function for the samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : ndarray of shape (n_samples, n_classes * (n_classes-1) / 2)\n",
      "     |          Returns the decision function of the sample for each class\n",
      "     |          in the model.\n",
      "     |          If decision_function_shape='ovr', the shape is (n_samples,\n",
      "     |          n_classes).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If decision_function_shape='ovo', the function values are proportional\n",
      "     |      to the distance of the samples X to the separating hyperplane. If the\n",
      "     |      exact distances are required, divide the function values by the norm of\n",
      "     |      the weight vector (``coef_``). See also `this question\n",
      "     |      <https://stats.stackexchange.com/questions/14876/\n",
      "     |      interpreting-distance-from-hyperplane-in-svm>`_ for further details.\n",
      "     |      If decision_function_shape='ovr', the decision function is a monotonic\n",
      "     |      transformation of ovo decision function.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform classification on samples in X.\n",
      "     |      \n",
      "     |      For an one-class model, +1 or -1 is returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples_test, n_samples_train)\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples_test, n_samples_train).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,)\n",
      "     |          Class labels for samples in X.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.svm._base.BaseSVC:\n",
      "     |  \n",
      "     |  predict_log_proba\n",
      "     |      Compute log probabilities of possible outcomes for samples in X.\n",
      "     |      \n",
      "     |      The model need to have probability information computed at training\n",
      "     |      time: fit with attribute `probability` set to True.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features) or                 (n_samples_test, n_samples_train)\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples_test, n_samples_train).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : ndarray of shape (n_samples, n_classes)\n",
      "     |          Returns the log-probabilities of the sample for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The probability model is created using cross validation, so\n",
      "     |      the results can be slightly different than those obtained by\n",
      "     |      predict. Also, it will produce meaningless results on very small\n",
      "     |      datasets.\n",
      "     |  \n",
      "     |  predict_proba\n",
      "     |      Compute probabilities of possible outcomes for samples in X.\n",
      "     |      \n",
      "     |      The model need to have probability information computed at training\n",
      "     |      time: fit with attribute `probability` set to True.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples_test, n_samples_train).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : ndarray of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the sample for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The probability model is created using cross validation, so\n",
      "     |      the results can be slightly different than those obtained by\n",
      "     |      predict. Also, it will produce meaningless results on very small\n",
      "     |      datasets.\n",
      "     |  \n",
      "     |  probA_\n",
      "     |  \n",
      "     |  probB_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.svm._base.BaseLibSVM:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the SVM model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)                 or (n_samples, n_samples)\n",
      "     |          Training vectors, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples, n_samples).\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Per-sample weights. Rescale C per sample. Higher weights\n",
      "     |          force the classifier to put more emphasis on these points.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
      "     |      X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
      "     |      \n",
      "     |      If X is a dense array, then the other methods will not support sparse\n",
      "     |      matrices as input.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.svm._base.BaseLibSVM:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |  \n",
      "     |  n_support_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class NuSVR(sklearn.base.RegressorMixin, sklearn.svm._base.BaseLibSVM)\n",
      "     |  Nu Support Vector Regression.\n",
      "     |  \n",
      "     |  Similar to NuSVC, for regression, uses a parameter nu to control\n",
      "     |  the number of support vectors. However, unlike NuSVC, where nu\n",
      "     |  replaces C, here nu replaces the parameter epsilon of epsilon-SVR.\n",
      "     |  \n",
      "     |  The implementation is based on libsvm.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <svm_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  nu : float, default=0.5\n",
      "     |      An upper bound on the fraction of training errors and a lower bound of\n",
      "     |      the fraction of support vectors. Should be in the interval (0, 1].  By\n",
      "     |      default 0.5 will be taken.\n",
      "     |  \n",
      "     |  C : float, default=1.0\n",
      "     |      Penalty parameter C of the error term.\n",
      "     |  \n",
      "     |  kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\n",
      "     |       Specifies the kernel type to be used in the algorithm.\n",
      "     |       It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
      "     |       a callable.\n",
      "     |       If none is given, 'rbf' will be used. If a callable is given it is\n",
      "     |       used to precompute the kernel matrix.\n",
      "     |  \n",
      "     |  degree : int, default=3\n",
      "     |      Degree of the polynomial kernel function ('poly').\n",
      "     |      Ignored by all other kernels.\n",
      "     |  \n",
      "     |  gamma : {'scale', 'auto'} or float, default='scale'\n",
      "     |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
      "     |  \n",
      "     |      - if ``gamma='scale'`` (default) is passed then it uses\n",
      "     |        1 / (n_features * X.var()) as value of gamma,\n",
      "     |      - if 'auto', uses 1 / n_features.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |         The default value of ``gamma`` changed from 'auto' to 'scale'.\n",
      "     |  \n",
      "     |  coef0 : float, default=0.0\n",
      "     |      Independent term in kernel function.\n",
      "     |      It is only significant in 'poly' and 'sigmoid'.\n",
      "     |  \n",
      "     |  shrinking : bool, default=True\n",
      "     |      Whether to use the shrinking heuristic.\n",
      "     |      See the :ref:`User Guide <shrinking_svm>`.\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      Tolerance for stopping criterion.\n",
      "     |  \n",
      "     |  cache_size : float, default=200\n",
      "     |      Specify the size of the kernel cache (in MB).\n",
      "     |  \n",
      "     |  verbose : bool, default=False\n",
      "     |      Enable verbose output. Note that this setting takes advantage of a\n",
      "     |      per-process runtime setting in libsvm that, if enabled, may not work\n",
      "     |      properly in a multithreaded context.\n",
      "     |  \n",
      "     |  max_iter : int, default=-1\n",
      "     |      Hard limit on iterations within solver, or -1 for no limit.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  class_weight_ : ndarray of shape (n_classes,)\n",
      "     |      Multipliers of parameter C for each class.\n",
      "     |      Computed based on the ``class_weight`` parameter.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (1, n_features)\n",
      "     |      Weights assigned to the features (coefficients in the primal\n",
      "     |      problem). This is only available in the case of a linear kernel.\n",
      "     |  \n",
      "     |      `coef_` is readonly property derived from `dual_coef_` and\n",
      "     |      `support_vectors_`.\n",
      "     |  \n",
      "     |  dual_coef_ : ndarray of shape (1, n_SV)\n",
      "     |      Coefficients of the support vector in the decision function.\n",
      "     |  \n",
      "     |  fit_status_ : int\n",
      "     |      0 if correctly fitted, 1 otherwise (will raise warning)\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,)\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  n_support_ : ndarray of shape (n_classes,), dtype=int32\n",
      "     |      Number of support vectors for each class.\n",
      "     |  \n",
      "     |  shape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n",
      "     |      Array dimensions of training vector ``X``.\n",
      "     |  \n",
      "     |  support_ : ndarray of shape (n_SV,)\n",
      "     |      Indices of support vectors.\n",
      "     |  \n",
      "     |  support_vectors_ : ndarray of shape (n_SV, n_features)\n",
      "     |      Support vectors.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.svm import NuSVR\n",
      "     |  >>> from sklearn.pipeline import make_pipeline\n",
      "     |  >>> from sklearn.preprocessing import StandardScaler\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> n_samples, n_features = 10, 5\n",
      "     |  >>> np.random.seed(0)\n",
      "     |  >>> y = np.random.randn(n_samples)\n",
      "     |  >>> X = np.random.randn(n_samples, n_features)\n",
      "     |  >>> regr = make_pipeline(StandardScaler(), NuSVR(C=1.0, nu=0.1))\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "     |                  ('nusvr', NuSVR(nu=0.1))])\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  NuSVC : Support Vector Machine for classification implemented with libsvm\n",
      "     |      with a parameter to control the number of support vectors.\n",
      "     |  \n",
      "     |  SVR : Epsilon Support Vector Machine for regression implemented with\n",
      "     |      libsvm.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] `LIBSVM: A Library for Support Vector Machines\n",
      "     |      <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n",
      "     |  \n",
      "     |  .. [2] `Platt, John (1999). \"Probabilistic outputs for support vector\n",
      "     |      machines and comparison to regularizedlikelihood methods.\"\n",
      "     |      <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639>`_\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NuSVR\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.svm._base.BaseLibSVM\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, nu=0.5, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, tol=0.001, cache_size=200, verbose=False, max_iter=-1)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.svm._base.BaseLibSVM:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the SVM model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)                 or (n_samples, n_samples)\n",
      "     |          Training vectors, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples, n_samples).\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Per-sample weights. Rescale C per sample. Higher weights\n",
      "     |          force the classifier to put more emphasis on these points.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
      "     |      X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
      "     |      \n",
      "     |      If X is a dense array, then the other methods will not support sparse\n",
      "     |      matrices as input.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform regression on samples in X.\n",
      "     |      \n",
      "     |      For an one-class model, +1 (inlier) or -1 (outlier) is returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples_test, n_samples_train).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.svm._base.BaseLibSVM:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |  \n",
      "     |  n_support_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class OneClassSVM(sklearn.base.OutlierMixin, sklearn.svm._base.BaseLibSVM)\n",
      "     |  Unsupervised Outlier Detection.\n",
      "     |  \n",
      "     |  Estimate the support of a high-dimensional distribution.\n",
      "     |  \n",
      "     |  The implementation is based on libsvm.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <outlier_detection>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\n",
      "     |       Specifies the kernel type to be used in the algorithm.\n",
      "     |       It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
      "     |       a callable.\n",
      "     |       If none is given, 'rbf' will be used. If a callable is given it is\n",
      "     |       used to precompute the kernel matrix.\n",
      "     |  \n",
      "     |  degree : int, default=3\n",
      "     |      Degree of the polynomial kernel function ('poly').\n",
      "     |      Ignored by all other kernels.\n",
      "     |  \n",
      "     |  gamma : {'scale', 'auto'} or float, default='scale'\n",
      "     |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
      "     |  \n",
      "     |      - if ``gamma='scale'`` (default) is passed then it uses\n",
      "     |        1 / (n_features * X.var()) as value of gamma,\n",
      "     |      - if 'auto', uses 1 / n_features.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |         The default value of ``gamma`` changed from 'auto' to 'scale'.\n",
      "     |  \n",
      "     |  coef0 : float, default=0.0\n",
      "     |      Independent term in kernel function.\n",
      "     |      It is only significant in 'poly' and 'sigmoid'.\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      Tolerance for stopping criterion.\n",
      "     |  \n",
      "     |  nu : float, default=0.5\n",
      "     |      An upper bound on the fraction of training\n",
      "     |      errors and a lower bound of the fraction of support\n",
      "     |      vectors. Should be in the interval (0, 1]. By default 0.5\n",
      "     |      will be taken.\n",
      "     |  \n",
      "     |  shrinking : bool, default=True\n",
      "     |      Whether to use the shrinking heuristic.\n",
      "     |      See the :ref:`User Guide <shrinking_svm>`.\n",
      "     |  \n",
      "     |  cache_size : float, default=200\n",
      "     |      Specify the size of the kernel cache (in MB).\n",
      "     |  \n",
      "     |  verbose : bool, default=False\n",
      "     |      Enable verbose output. Note that this setting takes advantage of a\n",
      "     |      per-process runtime setting in libsvm that, if enabled, may not work\n",
      "     |      properly in a multithreaded context.\n",
      "     |  \n",
      "     |  max_iter : int, default=-1\n",
      "     |      Hard limit on iterations within solver, or -1 for no limit.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  class_weight_ : ndarray of shape (n_classes,)\n",
      "     |      Multipliers of parameter C for each class.\n",
      "     |      Computed based on the ``class_weight`` parameter.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (1, n_features)\n",
      "     |      Weights assigned to the features (coefficients in the primal\n",
      "     |      problem). This is only available in the case of a linear kernel.\n",
      "     |  \n",
      "     |      `coef_` is readonly property derived from `dual_coef_` and\n",
      "     |      `support_vectors_`.\n",
      "     |  \n",
      "     |  dual_coef_ : ndarray of shape (1, n_SV)\n",
      "     |      Coefficients of the support vectors in the decision function.\n",
      "     |  \n",
      "     |  fit_status_ : int\n",
      "     |      0 if correctly fitted, 1 otherwise (will raise warning)\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,)\n",
      "     |      Constant in the decision function.\n",
      "     |  \n",
      "     |  n_support_ : ndarray of shape (n_classes,), dtype=int32\n",
      "     |      Number of support vectors for each class.\n",
      "     |  \n",
      "     |  offset_ : float\n",
      "     |      Offset used to define the decision function from the raw scores.\n",
      "     |      We have the relation: decision_function = score_samples - `offset_`.\n",
      "     |      The offset is the opposite of `intercept_` and is provided for\n",
      "     |      consistency with other outlier detection algorithms.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  shape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n",
      "     |      Array dimensions of training vector ``X``.\n",
      "     |  \n",
      "     |  support_ : ndarray of shape (n_SV,)\n",
      "     |      Indices of support vectors.\n",
      "     |  \n",
      "     |  support_vectors_ : ndarray of shape (n_SV, n_features)\n",
      "     |      Support vectors.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.svm import OneClassSVM\n",
      "     |  >>> X = [[0], [0.44], [0.45], [0.46], [1]]\n",
      "     |  >>> clf = OneClassSVM(gamma='auto').fit(X)\n",
      "     |  >>> clf.predict(X)\n",
      "     |  array([-1,  1,  1,  1, -1])\n",
      "     |  >>> clf.score_samples(X)\n",
      "     |  array([1.7798..., 2.0547..., 2.0556..., 2.0561..., 1.7332...])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OneClassSVM\n",
      "     |      sklearn.base.OutlierMixin\n",
      "     |      sklearn.svm._base.BaseLibSVM\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, nu=0.5, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Signed distance to the separating hyperplane.\n",
      "     |      \n",
      "     |      Signed distance is positive for an inlier and negative for an outlier.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The data matrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      dec : ndarray of shape (n_samples,)\n",
      "     |          Returns the decision function of the samples.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, sample_weight=None, **params)\n",
      "     |      Detects the soft boundary of the set of samples X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Set of samples, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Per-sample weights. Rescale C per sample. Higher weights\n",
      "     |          force the classifier to put more emphasis on these points.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If X is not a C-ordered contiguous array it is copied.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform classification on samples in X.\n",
      "     |      \n",
      "     |      For a one-class model, +1 or -1 is returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples_test, n_samples_train)\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples_test, n_samples_train).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,)\n",
      "     |          Class labels for samples in X.\n",
      "     |  \n",
      "     |  score_samples(self, X)\n",
      "     |      Raw scoring function of the samples.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The data matrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score_samples : ndarray of shape (n_samples,)\n",
      "     |          Returns the (unshifted) scoring function of the samples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  probA_\n",
      "     |  \n",
      "     |  probB_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.OutlierMixin:\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None)\n",
      "     |      Perform fit on X and returns labels for X.\n",
      "     |      \n",
      "     |      Returns -1 for outliers and 1 for inliers.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix, dataframe} of shape             (n_samples, n_features)\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          1 for inliers, -1 for outliers.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.OutlierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.svm._base.BaseLibSVM:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |  \n",
      "     |  n_support_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class SVC(sklearn.svm._base.BaseSVC)\n",
      "     |  C-Support Vector Classification.\n",
      "     |  \n",
      "     |  The implementation is based on libsvm. The fit time scales at least\n",
      "     |  quadratically with the number of samples and may be impractical\n",
      "     |  beyond tens of thousands of samples. For large datasets\n",
      "     |  consider using :class:`~sklearn.svm.LinearSVC` or\n",
      "     |  :class:`~sklearn.linear_model.SGDClassifier` instead, possibly after a\n",
      "     |  :class:`~sklearn.kernel_approximation.Nystroem` transformer.\n",
      "     |  \n",
      "     |  The multiclass support is handled according to a one-vs-one scheme.\n",
      "     |  \n",
      "     |  For details on the precise mathematical formulation of the provided\n",
      "     |  kernel functions and how `gamma`, `coef0` and `degree` affect each\n",
      "     |  other, see the corresponding section in the narrative documentation:\n",
      "     |  :ref:`svm_kernels`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <svm_classification>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  C : float, default=1.0\n",
      "     |      Regularization parameter. The strength of the regularization is\n",
      "     |      inversely proportional to C. Must be strictly positive. The penalty\n",
      "     |      is a squared l2 penalty.\n",
      "     |  \n",
      "     |  kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\n",
      "     |      Specifies the kernel type to be used in the algorithm.\n",
      "     |      It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
      "     |      a callable.\n",
      "     |      If none is given, 'rbf' will be used. If a callable is given it is\n",
      "     |      used to pre-compute the kernel matrix from data matrices; that matrix\n",
      "     |      should be an array of shape ``(n_samples, n_samples)``.\n",
      "     |  \n",
      "     |  degree : int, default=3\n",
      "     |      Degree of the polynomial kernel function ('poly').\n",
      "     |      Ignored by all other kernels.\n",
      "     |  \n",
      "     |  gamma : {'scale', 'auto'} or float, default='scale'\n",
      "     |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
      "     |  \n",
      "     |      - if ``gamma='scale'`` (default) is passed then it uses\n",
      "     |        1 / (n_features * X.var()) as value of gamma,\n",
      "     |      - if 'auto', uses 1 / n_features.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |         The default value of ``gamma`` changed from 'auto' to 'scale'.\n",
      "     |  \n",
      "     |  coef0 : float, default=0.0\n",
      "     |      Independent term in kernel function.\n",
      "     |      It is only significant in 'poly' and 'sigmoid'.\n",
      "     |  \n",
      "     |  shrinking : bool, default=True\n",
      "     |      Whether to use the shrinking heuristic.\n",
      "     |      See the :ref:`User Guide <shrinking_svm>`.\n",
      "     |  \n",
      "     |  probability : bool, default=False\n",
      "     |      Whether to enable probability estimates. This must be enabled prior\n",
      "     |      to calling `fit`, will slow down that method as it internally uses\n",
      "     |      5-fold cross-validation, and `predict_proba` may be inconsistent with\n",
      "     |      `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      Tolerance for stopping criterion.\n",
      "     |  \n",
      "     |  cache_size : float, default=200\n",
      "     |      Specify the size of the kernel cache (in MB).\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', default=None\n",
      "     |      Set the parameter C of class i to class_weight[i]*C for\n",
      "     |      SVC. If not given, all classes are supposed to have\n",
      "     |      weight one.\n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |  verbose : bool, default=False\n",
      "     |      Enable verbose output. Note that this setting takes advantage of a\n",
      "     |      per-process runtime setting in libsvm that, if enabled, may not work\n",
      "     |      properly in a multithreaded context.\n",
      "     |  \n",
      "     |  max_iter : int, default=-1\n",
      "     |      Hard limit on iterations within solver, or -1 for no limit.\n",
      "     |  \n",
      "     |  decision_function_shape : {'ovo', 'ovr'}, default='ovr'\n",
      "     |      Whether to return a one-vs-rest ('ovr') decision function of shape\n",
      "     |      (n_samples, n_classes) as all other classifiers, or the original\n",
      "     |      one-vs-one ('ovo') decision function of libsvm which has shape\n",
      "     |      (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one\n",
      "     |      ('ovo') is always used as multi-class strategy. The parameter is\n",
      "     |      ignored for binary classification.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.19\n",
      "     |          decision_function_shape is 'ovr' by default.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *decision_function_shape='ovr'* is recommended.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.17\n",
      "     |         Deprecated *decision_function_shape='ovo' and None*.\n",
      "     |  \n",
      "     |  break_ties : bool, default=False\n",
      "     |      If true, ``decision_function_shape='ovr'``, and number of classes > 2,\n",
      "     |      :term:`predict` will break ties according to the confidence values of\n",
      "     |      :term:`decision_function`; otherwise the first class among the tied\n",
      "     |      classes is returned. Please note that breaking ties comes at a\n",
      "     |      relatively high computational cost compared to a simple predict.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the pseudo random number generation for shuffling the data for\n",
      "     |      probability estimates. Ignored when `probability` is False.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  class_weight_ : ndarray of shape (n_classes,)\n",
      "     |      Multipliers of parameter C for each class.\n",
      "     |      Computed based on the ``class_weight`` parameter.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The classes labels.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)\n",
      "     |      Weights assigned to the features (coefficients in the primal\n",
      "     |      problem). This is only available in the case of a linear kernel.\n",
      "     |  \n",
      "     |      `coef_` is a readonly property derived from `dual_coef_` and\n",
      "     |      `support_vectors_`.\n",
      "     |  \n",
      "     |  dual_coef_ : ndarray of shape (n_classes -1, n_SV)\n",
      "     |      Dual coefficients of the support vector in the decision\n",
      "     |      function (see :ref:`sgd_mathematical_formulation`), multiplied by\n",
      "     |      their targets.\n",
      "     |      For multiclass, coefficient for all 1-vs-1 classifiers.\n",
      "     |      The layout of the coefficients in the multiclass case is somewhat\n",
      "     |      non-trivial. See the :ref:`multi-class section of the User Guide\n",
      "     |      <svm_multi_class>` for details.\n",
      "     |  \n",
      "     |  fit_status_ : int\n",
      "     |      0 if correctly fitted, 1 otherwise (will raise warning)\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  support_ : ndarray of shape (n_SV)\n",
      "     |      Indices of support vectors.\n",
      "     |  \n",
      "     |  support_vectors_ : ndarray of shape (n_SV, n_features)\n",
      "     |      Support vectors.\n",
      "     |  \n",
      "     |  n_support_ : ndarray of shape (n_classes,), dtype=int32\n",
      "     |      Number of support vectors for each class.\n",
      "     |  \n",
      "     |  probA_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n",
      "     |  probB_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n",
      "     |      If `probability=True`, it corresponds to the parameters learned in\n",
      "     |      Platt scaling to produce probability estimates from decision values.\n",
      "     |      If `probability=False`, it's an empty array. Platt scaling uses the\n",
      "     |      logistic function\n",
      "     |      ``1 / (1 + exp(decision_value * probA_ + probB_))``\n",
      "     |      where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\n",
      "     |      more information on the multiclass case and training procedure see\n",
      "     |      section 8 of [1]_.\n",
      "     |  \n",
      "     |  shape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n",
      "     |      Array dimensions of training vector ``X``.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.pipeline import make_pipeline\n",
      "     |  >>> from sklearn.preprocessing import StandardScaler\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      "     |  >>> y = np.array([1, 1, 2, 2])\n",
      "     |  >>> from sklearn.svm import SVC\n",
      "     |  >>> clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "     |                  ('svc', SVC(gamma='auto'))])\n",
      "     |  \n",
      "     |  >>> print(clf.predict([[-0.8, -1]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  SVR : Support Vector Machine for Regression implemented using libsvm.\n",
      "     |  \n",
      "     |  LinearSVC : Scalable Linear Support Vector Machine for classification\n",
      "     |      implemented using liblinear. Check the See Also section of\n",
      "     |      LinearSVC for more comparison element.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] `LIBSVM: A Library for Support Vector Machines\n",
      "     |      <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n",
      "     |  \n",
      "     |  .. [2] `Platt, John (1999). \"Probabilistic outputs for support vector\n",
      "     |      machines and comparison to regularizedlikelihood methods.\"\n",
      "     |      <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639>`_\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SVC\n",
      "     |      sklearn.svm._base.BaseSVC\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.svm._base.BaseLibSVM\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.svm._base.BaseSVC:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Evaluates the decision function for the samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : ndarray of shape (n_samples, n_classes * (n_classes-1) / 2)\n",
      "     |          Returns the decision function of the sample for each class\n",
      "     |          in the model.\n",
      "     |          If decision_function_shape='ovr', the shape is (n_samples,\n",
      "     |          n_classes).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If decision_function_shape='ovo', the function values are proportional\n",
      "     |      to the distance of the samples X to the separating hyperplane. If the\n",
      "     |      exact distances are required, divide the function values by the norm of\n",
      "     |      the weight vector (``coef_``). See also `this question\n",
      "     |      <https://stats.stackexchange.com/questions/14876/\n",
      "     |      interpreting-distance-from-hyperplane-in-svm>`_ for further details.\n",
      "     |      If decision_function_shape='ovr', the decision function is a monotonic\n",
      "     |      transformation of ovo decision function.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform classification on samples in X.\n",
      "     |      \n",
      "     |      For an one-class model, +1 or -1 is returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples_test, n_samples_train)\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples_test, n_samples_train).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,)\n",
      "     |          Class labels for samples in X.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.svm._base.BaseSVC:\n",
      "     |  \n",
      "     |  predict_log_proba\n",
      "     |      Compute log probabilities of possible outcomes for samples in X.\n",
      "     |      \n",
      "     |      The model need to have probability information computed at training\n",
      "     |      time: fit with attribute `probability` set to True.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features) or                 (n_samples_test, n_samples_train)\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples_test, n_samples_train).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : ndarray of shape (n_samples, n_classes)\n",
      "     |          Returns the log-probabilities of the sample for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The probability model is created using cross validation, so\n",
      "     |      the results can be slightly different than those obtained by\n",
      "     |      predict. Also, it will produce meaningless results on very small\n",
      "     |      datasets.\n",
      "     |  \n",
      "     |  predict_proba\n",
      "     |      Compute probabilities of possible outcomes for samples in X.\n",
      "     |      \n",
      "     |      The model need to have probability information computed at training\n",
      "     |      time: fit with attribute `probability` set to True.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples_test, n_samples_train).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : ndarray of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the sample for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The probability model is created using cross validation, so\n",
      "     |      the results can be slightly different than those obtained by\n",
      "     |      predict. Also, it will produce meaningless results on very small\n",
      "     |      datasets.\n",
      "     |  \n",
      "     |  probA_\n",
      "     |  \n",
      "     |  probB_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.svm._base.BaseLibSVM:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the SVM model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)                 or (n_samples, n_samples)\n",
      "     |          Training vectors, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples, n_samples).\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Per-sample weights. Rescale C per sample. Higher weights\n",
      "     |          force the classifier to put more emphasis on these points.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
      "     |      X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
      "     |      \n",
      "     |      If X is a dense array, then the other methods will not support sparse\n",
      "     |      matrices as input.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.svm._base.BaseLibSVM:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |  \n",
      "     |  n_support_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class SVR(sklearn.base.RegressorMixin, sklearn.svm._base.BaseLibSVM)\n",
      "     |  Epsilon-Support Vector Regression.\n",
      "     |  \n",
      "     |  The free parameters in the model are C and epsilon.\n",
      "     |  \n",
      "     |  The implementation is based on libsvm. The fit time complexity\n",
      "     |  is more than quadratic with the number of samples which makes it hard\n",
      "     |  to scale to datasets with more than a couple of 10000 samples. For large\n",
      "     |  datasets consider using :class:`~sklearn.svm.LinearSVR` or\n",
      "     |  :class:`~sklearn.linear_model.SGDRegressor` instead, possibly after a\n",
      "     |  :class:`~sklearn.kernel_approximation.Nystroem` transformer.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <svm_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\n",
      "     |       Specifies the kernel type to be used in the algorithm.\n",
      "     |       It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
      "     |       a callable.\n",
      "     |       If none is given, 'rbf' will be used. If a callable is given it is\n",
      "     |       used to precompute the kernel matrix.\n",
      "     |  \n",
      "     |  degree : int, default=3\n",
      "     |      Degree of the polynomial kernel function ('poly').\n",
      "     |      Ignored by all other kernels.\n",
      "     |  \n",
      "     |  gamma : {'scale', 'auto'} or float, default='scale'\n",
      "     |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
      "     |  \n",
      "     |      - if ``gamma='scale'`` (default) is passed then it uses\n",
      "     |        1 / (n_features * X.var()) as value of gamma,\n",
      "     |      - if 'auto', uses 1 / n_features.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |         The default value of ``gamma`` changed from 'auto' to 'scale'.\n",
      "     |  \n",
      "     |  coef0 : float, default=0.0\n",
      "     |      Independent term in kernel function.\n",
      "     |      It is only significant in 'poly' and 'sigmoid'.\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      Tolerance for stopping criterion.\n",
      "     |  \n",
      "     |  C : float, default=1.0\n",
      "     |      Regularization parameter. The strength of the regularization is\n",
      "     |      inversely proportional to C. Must be strictly positive.\n",
      "     |      The penalty is a squared l2 penalty.\n",
      "     |  \n",
      "     |  epsilon : float, default=0.1\n",
      "     |       Epsilon in the epsilon-SVR model. It specifies the epsilon-tube\n",
      "     |       within which no penalty is associated in the training loss function\n",
      "     |       with points predicted within a distance epsilon from the actual\n",
      "     |       value.\n",
      "     |  \n",
      "     |  shrinking : bool, default=True\n",
      "     |      Whether to use the shrinking heuristic.\n",
      "     |      See the :ref:`User Guide <shrinking_svm>`.\n",
      "     |  \n",
      "     |  cache_size : float, default=200\n",
      "     |      Specify the size of the kernel cache (in MB).\n",
      "     |  \n",
      "     |  verbose : bool, default=False\n",
      "     |      Enable verbose output. Note that this setting takes advantage of a\n",
      "     |      per-process runtime setting in libsvm that, if enabled, may not work\n",
      "     |      properly in a multithreaded context.\n",
      "     |  \n",
      "     |  max_iter : int, default=-1\n",
      "     |      Hard limit on iterations within solver, or -1 for no limit.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  class_weight_ : ndarray of shape (n_classes,)\n",
      "     |      Multipliers of parameter C for each class.\n",
      "     |      Computed based on the ``class_weight`` parameter.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (1, n_features)\n",
      "     |      Weights assigned to the features (coefficients in the primal\n",
      "     |      problem). This is only available in the case of a linear kernel.\n",
      "     |  \n",
      "     |      `coef_` is readonly property derived from `dual_coef_` and\n",
      "     |      `support_vectors_`.\n",
      "     |  \n",
      "     |  dual_coef_ : ndarray of shape (1, n_SV)\n",
      "     |      Coefficients of the support vector in the decision function.\n",
      "     |  \n",
      "     |  fit_status_ : int\n",
      "     |      0 if correctly fitted, 1 otherwise (will raise warning)\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,)\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  n_support_ : ndarray of shape (n_classes,), dtype=int32\n",
      "     |      Number of support vectors for each class.\n",
      "     |  \n",
      "     |  shape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n",
      "     |      Array dimensions of training vector ``X``.\n",
      "     |  \n",
      "     |  support_ : ndarray of shape (n_SV,)\n",
      "     |      Indices of support vectors.\n",
      "     |  \n",
      "     |  support_vectors_ : ndarray of shape (n_SV, n_features)\n",
      "     |      Support vectors.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.svm import SVR\n",
      "     |  >>> from sklearn.pipeline import make_pipeline\n",
      "     |  >>> from sklearn.preprocessing import StandardScaler\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> n_samples, n_features = 10, 5\n",
      "     |  >>> rng = np.random.RandomState(0)\n",
      "     |  >>> y = rng.randn(n_samples)\n",
      "     |  >>> X = rng.randn(n_samples, n_features)\n",
      "     |  >>> regr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "     |                  ('svr', SVR(epsilon=0.2))])\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  NuSVR : Support Vector Machine for regression implemented using libsvm\n",
      "     |      using a parameter to control the number of support vectors.\n",
      "     |  \n",
      "     |  LinearSVR : Scalable Linear Support Vector Machine for regression\n",
      "     |      implemented using liblinear.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] `LIBSVM: A Library for Support Vector Machines\n",
      "     |      <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n",
      "     |  \n",
      "     |  .. [2] `Platt, John (1999). \"Probabilistic outputs for support vector\n",
      "     |      machines and comparison to regularizedlikelihood methods.\"\n",
      "     |      <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639>`_\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SVR\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.svm._base.BaseLibSVM\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  probA_\n",
      "     |  \n",
      "     |  probB_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.svm._base.BaseLibSVM:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the SVM model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)                 or (n_samples, n_samples)\n",
      "     |          Training vectors, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples, n_samples).\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Per-sample weights. Rescale C per sample. Higher weights\n",
      "     |          force the classifier to put more emphasis on these points.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
      "     |      X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
      "     |      \n",
      "     |      If X is a dense array, then the other methods will not support sparse\n",
      "     |      matrices as input.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform regression on samples in X.\n",
      "     |      \n",
      "     |      For an one-class model, +1 (inlier) or -1 (outlier) is returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples_test, n_samples_train).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.svm._base.BaseLibSVM:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |  \n",
      "     |  n_support_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "\n",
      "FUNCTIONS\n",
      "    l1_min_c(X, y, *, loss='squared_hinge', fit_intercept=True, intercept_scaling=1.0)\n",
      "        Return the lowest bound for C such that for C in (l1_min_C, infinity)\n",
      "        the model is guaranteed not to be empty. This applies to l1 penalized\n",
      "        classifiers, such as LinearSVC with penalty='l1' and\n",
      "        linear_model.LogisticRegression with penalty='l1'.\n",
      "        \n",
      "        This value is valid if class_weight parameter in fit() is not set.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Training vector, where n_samples in the number of samples and\n",
      "            n_features is the number of features.\n",
      "        \n",
      "        y : array-like of shape (n_samples,)\n",
      "            Target vector relative to X.\n",
      "        \n",
      "        loss : {'squared_hinge', 'log'}, default='squared_hinge'\n",
      "            Specifies the loss function.\n",
      "            With 'squared_hinge' it is the squared hinge loss (a.k.a. L2 loss).\n",
      "            With 'log' it is the loss of logistic regression models.\n",
      "        \n",
      "        fit_intercept : bool, default=True\n",
      "            Specifies if the intercept should be fitted by the model.\n",
      "            It must match the fit() method parameter.\n",
      "        \n",
      "        intercept_scaling : float, default=1.0\n",
      "            when fit_intercept is True, instance vector x becomes\n",
      "            [x, intercept_scaling],\n",
      "            i.e. a \"synthetic\" feature with constant value equals to\n",
      "            intercept_scaling is appended to the instance vector.\n",
      "            It must match the fit() method parameter.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        l1_min_c : float\n",
      "            minimum value for C\n",
      "\n",
      "DATA\n",
      "    __all__ = ['LinearSVC', 'LinearSVR', 'NuSVC', 'NuSVR', 'OneClassSVM', ...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\archit\\anaconda3\\envs\\ailabs\\lib\\site-packages\\sklearn\\svm\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv1=svm.SVC()\n",
    "# help(sv1)\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score : 0.8214805689326913\n",
      "Test score : 0.8316166592028661\n",
      "time Taken 8.171918153762817 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "st=time.time()\n",
    "sv1.fit(X_train_sc_df, y_train)\n",
    "print(\"Train score :\", sv1.score(X_train_sc_df, y_train))\n",
    "print(\"Test score :\", sv1.score(X_test_sc_df, y_test))\n",
    "et=time.time()\n",
    "print(\"time Taken\", et-st, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv1.predict(X_train_sc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "predict_proba is not available when  probability=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-e765d465599e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_sc_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\AIlabs\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    664\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m         \"\"\"\n\u001b[1;32m--> 666\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    667\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_proba\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AIlabs\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_check_proba\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m             raise AttributeError(\"predict_proba is not available when \"\n\u001b[0m\u001b[0;32m    634\u001b[0m                                  \" probability=False\")\n\u001b[0;32m    635\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'c_svc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'nu_svc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: predict_proba is not available when  probability=False"
     ]
    }
   ],
   "source": [
    "sv1.predict_proba(X_train_sc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score : 0.8214805689326913\n",
      "Test score : 0.8316166592028661\n",
      "time Taken 20.588422775268555 sec\n"
     ]
    }
   ],
   "source": [
    "sv1=svm.SVC(probability=True)\n",
    "st=time.time()\n",
    "sv1.fit(X_train_sc_df, y_train)\n",
    "print(\"Train score :\", sv1.score(X_train_sc_df, y_train))\n",
    "print(\"Test score :\", sv1.score(X_test_sc_df, y_test))\n",
    "et=time.time()\n",
    "print(\"time Taken\", et-st, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score : 0.7905700526374734\n",
      "Test score : 0.8110165696372593\n",
      "time Taken 5.943622350692749 sec\n"
     ]
    }
   ],
   "source": [
    "sv2=svm.SVC(kernel='linear')\n",
    "st=time.time()\n",
    "sv2.fit(X_train_sc_df, y_train)\n",
    "print(\"Train score :\", sv2.score(X_train_sc_df, y_train))\n",
    "print(\"Test score :\", sv2.score(X_test_sc_df, y_test))\n",
    "et=time.time()\n",
    "print(\"time Taken\", et-st, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score : 0.7911300257587636\n",
      "Test score : 0.8110165696372593\n",
      "Time Taken 13.103582382202148 sec\n"
     ]
    }
   ],
   "source": [
    "st=time.time()\n",
    "\n",
    "sv2=svm.SVC(kernel=\"linear\", C=10)\n",
    "sv2.fit(X_train_sc_df, y_train)\n",
    "print(\"Train score :\", sv2.score(X_train_sc_df, y_train))\n",
    "print(\"Test score :\", sv2.score(X_test_sc_df, y_test))\n",
    "et=time.time()\n",
    "print(\"Time Taken\", et-st, \"sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score : 0.7911300257587636\n",
      "Test score : 0.8110165696372593\n",
      "Time Taken 63.15420079231262 sec\n"
     ]
    }
   ],
   "source": [
    "st=time.time()\n",
    "sv2=svm.SVC(kernel=\"linear\", C=10, probability=True)\n",
    "sv2.fit(X_train_sc_df, y_train)\n",
    "print(\"Train score :\", sv2.score(X_train_sc_df, y_train))\n",
    "print(\"Test score :\", sv2.score(X_test_sc_df, y_test))\n",
    "et=time.time()\n",
    "print(\"Time Taken\", et-st, \"sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sv2.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score : 0.7331168103931012\n",
      "Test score : 0.729511867442902\n",
      "Time Taken 7.298057794570923 sec\n"
     ]
    }
   ],
   "source": [
    "st=time.time()\n",
    "sv2=svm.SVC(kernel=\"poly\", degree=2)\n",
    "sv2.fit(X_train_sc_df, y_train)\n",
    "print(\"Train score :\", sv2.score(X_train_sc_df, y_train))\n",
    "print(\"Test score :\", sv2.score(X_test_sc_df, y_test))\n",
    "et=time.time()\n",
    "print(\"Time Taken\", et-st, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score : 0.8163288162168216\n",
      "Test score : 0.8114643976712942\n",
      "Time Taken 6.864281415939331 sec\n"
     ]
    }
   ],
   "source": [
    "st=time.time()\n",
    "sv2=svm.SVC(kernel=\"poly\", degree=3)\n",
    "sv2.fit(X_train_sc_df, y_train)\n",
    "print(\"Train score :\", sv2.score(X_train_sc_df, y_train))\n",
    "print(\"Test score :\", sv2.score(X_test_sc_df, y_test))\n",
    "et=time.time()\n",
    "print(\"Time Taken\", et-st, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score : 0.8163288162168216\n",
      "Test score : 0.8114643976712942\n"
     ]
    }
   ],
   "source": [
    "sv2=svm.SVC(kernel=\"poly\", degree=3, probability=True)\n",
    "sv2.fit(X_train_sc_df, y_train)\n",
    "print(\"Train score :\", sv2.score(X_train_sc_df, y_train))\n",
    "print(\"Test score :\", sv2.score(X_test_sc_df, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv2.predict(X_train_sc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21642325, 0.78357675],\n",
       "       [0.44388339, 0.55611661],\n",
       "       [0.84961766, 0.15038234],\n",
       "       ...,\n",
       "       [0.13568074, 0.86431926],\n",
       "       [0.48716612, 0.51283388],\n",
       "       [0.69160218, 0.30839782]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv2.predict_proba(X_train_sc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfhh.to_csv(r\"D:\\Training\\Imarticus\\PGA\\PGA15\\week10\\profile.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_decile(X,y,trained_model):\n",
    "    X_1=X.copy()\n",
    "    y_1=y.copy()\n",
    "    y_pred1=trained_model.predict(X_1)\n",
    "    X_1[\"Prob_Event\"]=trained_model.predict_proba(X_1)[:,1]\n",
    "    X_1[\"Y_actual\"]=y_1\n",
    "    X_1[\"Y_pred\"]=y_pred1\n",
    "    X_1[\"Rank\"]=pd.qcut(X_1[\"Prob_Event\"], 10, labels=np.arange(0,10,1))\n",
    "    X_1[\"numb\"]=10\n",
    "    X_1[\"Decile\"]=X_1[\"numb\"]-X_1[\"Rank\"].astype(\"int\")\n",
    "    \n",
    "    profile=pd.DataFrame(X_1.groupby(\"Decile\") \\\n",
    "                        .apply(lambda x: pd.Series({\n",
    "        'min_score'   : x[\"Prob_Event\"].min(),\n",
    "        'max_score'   : x[\"Prob_Event\"].max(),\n",
    "        'Event'       : x[\"Y_actual\"].sum(),\n",
    "        'Non_event'   : x[\"Y_actual\"].count()-x[\"Y_actual\"].sum(),\n",
    "        'Total'       : x[\"Y_actual\"].count() })))\n",
    "    return profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_score</th>\n",
       "      <th>max_score</th>\n",
       "      <th>Event</th>\n",
       "      <th>Non_event</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decile</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.055004e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>420.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>703.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.135451e-01</td>\n",
       "      <td>0.905443</td>\n",
       "      <td>432.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>703.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.189825e-01</td>\n",
       "      <td>0.813340</td>\n",
       "      <td>426.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>732.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.635493e-01</td>\n",
       "      <td>0.718858</td>\n",
       "      <td>402.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>692.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.126315e-01</td>\n",
       "      <td>0.563487</td>\n",
       "      <td>397.0</td>\n",
       "      <td>317.0</td>\n",
       "      <td>714.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.940495e-01</td>\n",
       "      <td>0.412133</td>\n",
       "      <td>425.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>720.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.163789e-01</td>\n",
       "      <td>0.293814</td>\n",
       "      <td>434.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>742.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.736204e-01</td>\n",
       "      <td>0.216379</td>\n",
       "      <td>426.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>707.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.250898e-01</td>\n",
       "      <td>0.173610</td>\n",
       "      <td>444.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>720.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.125011</td>\n",
       "      <td>418.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>725.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           min_score  max_score  Event  Non_event  Total\n",
       "Decile                                                  \n",
       "1       9.055004e-01   1.000000  420.0      283.0  703.0\n",
       "2       8.135451e-01   0.905443  432.0      271.0  703.0\n",
       "3       7.189825e-01   0.813340  426.0      306.0  732.0\n",
       "4       5.635493e-01   0.718858  402.0      290.0  692.0\n",
       "5       4.126315e-01   0.563487  397.0      317.0  714.0\n",
       "6       2.940495e-01   0.412133  425.0      295.0  720.0\n",
       "7       2.163789e-01   0.293814  434.0      308.0  742.0\n",
       "8       1.736204e-01   0.216379  426.0      281.0  707.0\n",
       "9       1.250898e-01   0.173610  444.0      276.0  720.0\n",
       "10      1.000000e-07   0.125011  418.0      307.0  725.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_decile(X_train_sc_df,y_train,sv2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1620"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*3*3*6*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=SVC(), n_jobs=-1,\n",
       "                   param_distributions={'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                                        'degree': [2, 3, 4],\n",
       "                                        'gamma': [0.1, 1, 0.001],\n",
       "                                        'kernel': ['linear', 'poly', 'rbf']},\n",
       "                   verbose=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params={\"kernel\":['linear', 'poly', 'rbf'],\n",
    "       \"degree\":[2,3,4],\n",
    "       \"gamma\":[0.1, 1,.001],\n",
    "       \"C\":[0.001, 0.01, 0.1, 1, 10, 100]\n",
    "       }\n",
    "sv=svm.SVC()\n",
    "svm_rs=RandomizedSearchCV(sv, params, cv=10, n_jobs=-1, verbose=True)\n",
    "svm_rs.fit(X_train_sc_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': 'rbf', 'gamma': 0.1, 'degree': 3, 'C': 1}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_rs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(), n_jobs=-1,\n",
       "             param_grid={'C': [10, 100], 'degree': [2, 3, 4],\n",
       "                         'gamma': [0.1, 0.01],\n",
       "                         'kernel': ['linear', 'poly', 'rbf']},\n",
       "             verbose=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params={\"kernel\":['linear', 'poly', 'rbf'],\n",
    "       \"degree\":[2,3, 4],\n",
    "       \"gamma\":[0.1,.01],\n",
    "       \"C\":[10, 100]\n",
    "       }\n",
    "sv=svm.SVC()\n",
    "\n",
    "sv_gs=GridSearchCV(sv, param_grid=params, cv=5, n_jobs=-1, verbose=True)\n",
    "sv_gs.fit(X_train_sc_df, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'degree': 2, 'gamma': 0.01, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score : 0.814536902228693\n",
      "Test score : 0.8307210031347962\n"
     ]
    }
   ],
   "source": [
    "sv2=svm.SVC(kernel=\"rbf\", degree=2,gamma=0.01, C=10, probability=True)\n",
    "sv2.fit(X_train_sc_df, y_train)\n",
    "print(\"Train score :\", sv2.score(X_train_sc_df, y_train))\n",
    "print(\"Test score :\", sv2.score(X_test_sc_df, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
